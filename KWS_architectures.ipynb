{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f58f176",
   "metadata": {},
   "source": [
    "# Keyword Spotting with different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341fb838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 20:14:48.548545: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-15 20:14:48.750950: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-15 20:14:48.757477: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-15 20:14:48.757494: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-15 20:14:49.448315: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-15 20:14:49.448395: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-15 20:14:49.448403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "\n",
    "from os.path import join as pjoin\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from utils import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f869b",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d659152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/mnt/speechdataset/processed_data'\n",
    "keywords = ['bed', 'down', 'forward', 'house', 'nine', 'one', 'six', 'tree']\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = load_dataset(data_dir, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f56c410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 39, 19587) (19587,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b65bae",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ca974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KWS_CNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=94, kernel_size=(66,8), strides = (1,1), \n",
    "                                            padding = 'valid', kernel_initializer = glorot_uniform(seed=0))\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=94, kernel_size=(20,4), strides = (1,1), \n",
    "                               padding = 'valid', kernel_initializer = glorot_uniform(seed=0))\n",
    "        self.maxpool = tf.keras.layers.MaxPooling2D(pool_size=(2,3), strides=(2,3), padding='valid')\n",
    "        self.act = tf.keras.layers.Activation('relu')\n",
    "        self.lin = tf.keras.layers.Dense(99)\n",
    "        self.dens = tf.keras.layers.Dense(396, activation='sigmoid')\n",
    "        self.softmx = tf.keras.layers.Softmax()\n",
    "        self.flat = tf.keras.layers.Flatten()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "    \n",
    "    # forward pass\n",
    "    def call(self, inputs, dropout=False):\n",
    "        x = self.conv1(inputs)\n",
    "        #if dropout:\n",
    "            #x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        x = self.dens(x)   \n",
    "        return self.softmx(x)\n",
    "    \n",
    "    #loss\n",
    "    def compute_loss(self, x, y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "decd1fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KWS_CNN_model(input_shape):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    :param input_shape: shape of the data of the dataset\n",
    "\n",
    "    :returns Model: a tf.keras.Model() instance\n",
    "    \"\"\"\n",
    "    \n",
    "    X_input = tf.keras.Input(input_shape)\n",
    "    \n",
    "    # CONV -> pooling -> CONV -> lin -> Dense?\n",
    "    # First convolution\n",
    "    X = tf.keras.layers.Conv2D(filters=94, kernel_size=(66,8), strides = (1,1), \n",
    "               padding = 'same', kernel_initializer = glorot_uniform(seed=0))(X_input)\n",
    "    \n",
    "    #Pooling on time and frequency\n",
    "    X = tf.keras.layers.MaxPooling2D(pool_size=(2,3), strides=(2,3), padding='valid')(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    #Second convolution\n",
    "    X = tf.keras.layers.Conv2D(filters=94, kernel_size=(20,4), strides = (1,1), \n",
    "                               padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    # Linear layer\n",
    "    X = tf.keras.layers.Flatten()(X)\n",
    "    X = tf.keras.layers.Dense(32)(X)\n",
    "    \n",
    "    # Dense layer\n",
    "    X = tf.keras.layers.Dense(128, activation='sigmoid')(X)\n",
    "    \n",
    "    # Softmax\n",
    "    X = tf.keras.layers.Softmax()(X)\n",
    "    \n",
    "    # MODEL\n",
    "    model = Model(inputs = X_input, outputs = X, name='KWS_CNN')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8990a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KWS_CNN_model((99,39,1))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b864d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Creating variables on a non-first call to a function decorated with tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m early_stop_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Fit\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:921\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    919\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    920\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    926\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Creating variables on a non-first call to a function decorated with tf.function."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 30\n",
    "\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Fit\n",
    "history = model.fit(np.transpose(X_train, [2,0,1]), Y_train, epochs=num_epochs, \n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b9baa25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19587, 99, 39)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(X_train, [2,0,1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131055a0",
   "metadata": {},
   "source": [
    "## Clustering, RNN, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778417b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
